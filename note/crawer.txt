robot.txt
	一个放在网站根目录的文件，告诉爬虫哪些内容可以爬取，哪些不能
	相当于一个君子协议，但基本没啥用，一些普通的网站基本都不会有这个，就算
	有也是禁止所有爬虫爬取，
	还有爬虫也不一定会遵守
爬虫的基本结构
	种子url
		一个或多个初始化url
	url队列管理
		有参数可以设置，当队列没有url时是阻塞还是停止setExitWhenComplete
		待爬取队列
			爬取策略决定了url如何排列顺序
				深度优先
					一个链接一个链接地跟踪下去，直到此路线完毕，然后爬取第
					个链接，遇到的问题会是爬虫无限深入，无法结束，可以在页
					面抽取的时候控制深度或者挑选挑选指定的url
				广度优先
					按层次进行搜索，如果此层没有搜索完成不会进入下一层，搜
					索有点盲目，会搜索整张图
				PageRank策略
				OPIC策略
				大站优先
				引用链接
		已爬取队列
			url去重
				hashset
					url数量不多的时候
				布隆过滤器
					没出现过	一定没出现过
					出现过		有一定的不确定性,但内存足够大的话这个不确
								定性可以变得很小
		将url加入队列的时候可以设置一些过滤条件
			比如url的深度，如果深度超过了某值就拒绝加入,从而避免url队列无限增长
	url下载器
		httpClient
		代理ip
	页面解析
		不同页面需要不同的解析器
		一般框架都会把down返回的html页面构造成一棵Dom树，然后可以通过一些
		选择器或者xpath获取页面的指定元素
			获取该节点下所有链接(返回的是一个List<String>)
			page.getHtml().$("#listnav > ul").links().all();
			获取该节点下的内容(返回的是文本)
			//*[@id="ip_list"]/tbody/tr/td[1]/text()
		解析也有两个
			页面内容解析
				从html页面中解析出自己需要的内容
			页面链接解析
				从html页面中解析出需要处理的url
				在此步骤可以控制是否继续爬取，通过在request中设置爬取深度的值，然后将解析出来的url的深度加一，然后发送给队列管理器
		在返回的html结果中选出自己需要的链接
		然后发送给Scheduler队列管理器
		发送的链接如果需要cookie是通过Site对象添加的
		
		对于不同的页面可以有不同的解析器，
		因为页面解析也是通过xpath或者selector获取特定节点的数据
		每个解析特定的页面，然后使用职责链模式串起来
		爬取深度问题
		如何结束
			爬取结束这里需要自己实现，在python的scrapy中有几个参数可以参考一下
				在抓取了指定item之后
				在收到指定数目响应之后
				在发生指定数目错误之后
		
	内容存储
		页面解析发送过来的数据都是相当于是一个HashMap对象，然后在文件存储中
		普通文件
		json数据
		数据库


	分布式需要关注的几个点
		队列管理
			所有的url都存储在一个队列中
			主机发送页面解析的url，获取待爬取的url
		爬取结果汇总
		
		每个机器相当于只要进行  页面下载，页面内容抽取



爬虫的种类
	垂直型爬虫
	宽度型爬虫

设置爬取深度
重复爬取问题
robots.txt
代理ip问题
url如何去重
url如何存储
爬虫url队列管理
	内存队列
	

	不支持暂停
	保存状态
	
	Downloader		url下载管理
	PageProcessor	页面提取
	Scheduler		url队列管理
		url的存储
			可以用java自身的一些线程安全的队列
			也可以外部存储
		url的顺序策略(也就是优先级的问题)
			可以是先来先进
		url的去重策略
			可以用hashset
		
	Pipeline		内容保存
	四个组件
	
	
爬虫怎么开始？
	通过一个指定的url去获取html页面，然后解析里面的链接，将链接加入队列，然后继通过这些链接获取html页面,如此循环
爬虫怎么结束？
	scrapy有几个参数
		在抓取了指定item之后
		在收到指定数目响应之后
		在发生指定数目错误之后
爬取策略？
	url存储策略
	深度优先
		从一个起始页，然后一个链接一个链接地深入进去，处理完这条线路后再处
		理另一条线路,有可能无线的搜索下去，所以可以设置爬取的深度，或者页面
		抽取链接的时候判断是否继续
	广度优先
		按层次进行搜索，如果此层没有搜索完成不会进入下一层
		搜索有点盲目，会彻底搜索整张图
	反向链接数策略
		一个url被被多少url
	
