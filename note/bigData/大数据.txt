1.在A上生成公钥私钥。 
2.将公钥拷贝给server B，要重命名成authorized_keys(从英文名就知道含义了) 
3.Server A向Server B发送一个连接请求。 
4.Server B得到Server A的信息后，在authorized_key中查找，如果有相应的用户名和IP，则随机生成一个字符串，并用Server A的公钥加密，发送给Server A。 
5.Server A得到Server B发来的消息后，使用私钥进行解密，然后将解密后的字符串发送给Server B。Server B进行和生成的对比，如果一致，则允许免登录。


authorized_keys文件中存储着本地系统可以允许远端计算机系统ssh免密码登陆的账号信息。也就是远端的计算机可以通过什么账号及地址不需要输入密码既可以远程登陆本系统。



hdfs安装所有节点必须在同一目录

truncate -s 0 filename   清空文件


办公网络密码:4E6B36A07DC2E5F57B0E724AAB
c_nienan密码:fcy,.!)FCY
运动:跑步
音乐:无





[root@master sbin]# ./start-dfs.sh
Starting namenodes on [master]
ERROR: Attempting to operate on hdfs namenode as root
ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.
Starting datanodes
ERROR: Attempting to operate on hdfs datanode as root
ERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation.
Starting secondary namenodes [slave1]
ERROR: Attempting to operate on hdfs secondarynamenode as root
ERROR: but there is no HDFS_SECONDARYNAMENODE_USER defined. Aborting operation.



将start-dfs.sh，stop-dfs.sh两个文件顶部添加以下参数

HDFS_DATANODE_USER=root
HDFS_DATANODE_SECURE_USER=hdfs
HDFS_NAMENODE_USER=root
HDFS_SECONDARYNAMENODE_USER=root



hdfs3.0版本不在是slaves文件了，而是workers文件









按文件大小查询
一个文件，上传到hdfs上时指定的是几个副本就是几个。以后你修改了副本数，对已经上传了的文件也不会起作用。可以再上传文件的同时指定创建的副本数
如果你只有3个datanode，但是你却指定副本数为4，是不会生效的，因为每个datanode上只能存放一个副本

hadoop3.0版本修改了副本数不重启也直接生效了
hadoop3.0版本NameNode的hdfs-site.xml中块大小的配置和DataNode的配置不一样，正常工作并且按照NameNode的配置生效

http://139.217.232.100:9870/dfshealth.html#tab-datanode





hadoop fsck / -files查看所有文件的信息

hadoop fsck / -files -blocks -locations -racks			文件信息，块信息，位置信息，机架信息

-list-corruptfileblocks		列出损坏的块信息